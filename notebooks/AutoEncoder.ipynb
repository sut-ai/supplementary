{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoEncoder.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Topic: AutoEncoder\n",
        "## Author: Atoosa Chegini"
      ],
      "metadata": {
        "id": "Z4S2v4kBZTzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoEncoder\n",
        "An autoencoder has two parts: an **encoder** and a **decoder**.\n",
        "\n",
        "\n",
        "The encoder reduces the dimensions of input data so that the original information is compressed.\n",
        "![Encoder](https://miro.medium.com/max/425/1*Dc3WI46OHvgu4c09nU4H4g.png)\n",
        "\n",
        "The decoder restores the original information from the compressed data.\n",
        "\n",
        "\n",
        "![Decoder](https://miro.medium.com/max/424/1*CO3-zpukfra4osABDfGc_Q.png)\n",
        "\n",
        "The autoencoder is a neural network that learns to encode and decode automatically. In this notebook we will learn how to implement a simple autoencoder and will see the application of this kind of Neural Network."
      ],
      "metadata": {
        "id": "QOi3tumx6560"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset here is **[MNIST](http://yann.lecun.com/exdb/mnist/)**, which is a well known database of handwritten digits. Keras has MNIST dataset utility. We can download the data as follows:"
      ],
      "metadata": {
        "id": "Zr2ge8-j8hlh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5eKa3ZBp5vb",
        "outputId": "85a4eddd-d6e7-449d-80fa-cc9399bd89bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "(X_train, _), (X_test, _) = keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the shape of each image in the dataset by using the code below."
      ],
      "metadata": {
        "id": "IEgghWzS9CmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The ourput is the shape of the image with index zero. (their shapes are all the same!)\n",
        "X_train[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pySz7ZM_8_bv",
        "outputId": "e81b1030-9f48-4aba-afb7-d15435c709a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.imshow(np.array(X_test[0], dtype='float').reshape((28, 28)), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "sdBrTjw5LRC6",
        "outputId": "e56d63d8-96c8-4344-e457-9917c61336ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Simple Autoencoder\n",
        "\n",
        "We start with a simple autoencoder based on a fully connected layers. One hidden layer handles the encoding, and the output layer handles the decoding.\n",
        "Each of the input images is flatten to an array of 784 (=28Ã—28) data points. This is then compressed into 32 data points by the fully connected layer."
      ],
      "metadata": {
        "id": "SgHTcG0K9vRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "inputs  = layers.Input(shape=(784,))           # 28*28 flatten\n",
        "enc_fc  = layers.Dense( 32, activation='relu') # to 32 data points\n",
        "encoded = enc_fc(inputs)"
      ],
      "metadata": {
        "id": "trLrHAq89sDj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we decode the encoded data to the original 784 data points. The sigmoid will return values between 0 and 1 for each pixel (intensity)."
      ],
      "metadata": {
        "id": "UyWtcFyH-KOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dec_fc  = layers.Dense(784, activation='sigmoid') # to 784 data points\n",
        "decoded = dec_fc(encoded)"
      ],
      "metadata": {
        "id": "CLqhCif7-Juz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This whole processing becomes the trainable autoencoder model."
      ],
      "metadata": {
        "id": "YsWc7V7x-QrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model \n",
        "autoencoder = Model(inputs, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "IbN7KJCR-FeV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we are going to show you what is the benefit of autoencoders. therefore, We preprocess the MNIST image data so that image data are normalized between 0 and 1."
      ],
      "metadata": {
        "id": "hgjV7PIV-v7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(x):\n",
        "    x = x.astype('float32') / 255.\n",
        "    return x.reshape(-1, np.prod(x.shape[1:])) # flatten\n",
        "X_train = preprocess(X_train)\n",
        "X_test  = preprocess(X_test)"
      ],
      "metadata": {
        "id": "hRLB-qcV-vWD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also split the train data into a train set and a validation set."
      ],
      "metadata": {
        "id": "7ArSRDNy_BR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# also create a validation set for training\n",
        "X_train, X_valid = train_test_split(X_train, test_size=500)"
      ],
      "metadata": {
        "id": "4Tt5OGL_-9s-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train the autoencoder which compress the input image and then restore to the original size. As such, our training data and label data are both the same image data."
      ],
      "metadata": {
        "id": "_dUDFJYy_OWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(X_train, X_train, # data and label are the same\n",
        "                epochs=50, \n",
        "                batch_size=128, \n",
        "                validation_data=(X_valid, X_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vvn-50RX-nGG",
        "outputId": "787a30a8-0799-4b0d-d61a-4e32c62128d0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "465/465 [==============================] - 4s 4ms/step - loss: 0.2286 - val_loss: 0.1623\n",
            "Epoch 2/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.1429 - val_loss: 0.1298\n",
            "Epoch 3/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.1207 - val_loss: 0.1137\n",
            "Epoch 4/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.1091 - val_loss: 0.1051\n",
            "Epoch 5/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.1026 - val_loss: 0.0998\n",
            "Epoch 6/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0985 - val_loss: 0.0963\n",
            "Epoch 7/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0963 - val_loss: 0.0948\n",
            "Epoch 8/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0952 - val_loss: 0.0939\n",
            "Epoch 9/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0946 - val_loss: 0.0935\n",
            "Epoch 10/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0943 - val_loss: 0.0932\n",
            "Epoch 11/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0940 - val_loss: 0.0930\n",
            "Epoch 12/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0938 - val_loss: 0.0929\n",
            "Epoch 13/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0937 - val_loss: 0.0927\n",
            "Epoch 14/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0936 - val_loss: 0.0926\n",
            "Epoch 15/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0934 - val_loss: 0.0927\n",
            "Epoch 16/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0933 - val_loss: 0.0925\n",
            "Epoch 17/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0933 - val_loss: 0.0925\n",
            "Epoch 18/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0932 - val_loss: 0.0926\n",
            "Epoch 19/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0931 - val_loss: 0.0925\n",
            "Epoch 20/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0931 - val_loss: 0.0925\n",
            "Epoch 21/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0931 - val_loss: 0.0925\n",
            "Epoch 22/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0930 - val_loss: 0.0925\n",
            "Epoch 23/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0930 - val_loss: 0.0924\n",
            "Epoch 24/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0930 - val_loss: 0.0924\n",
            "Epoch 25/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0929 - val_loss: 0.0923\n",
            "Epoch 26/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0929 - val_loss: 0.0925\n",
            "Epoch 27/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0929 - val_loss: 0.0924\n",
            "Epoch 28/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0929 - val_loss: 0.0923\n",
            "Epoch 29/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0928 - val_loss: 0.0923\n",
            "Epoch 30/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0928 - val_loss: 0.0924\n",
            "Epoch 31/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0928 - val_loss: 0.0923\n",
            "Epoch 32/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0928 - val_loss: 0.0923\n",
            "Epoch 33/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0928 - val_loss: 0.0924\n",
            "Epoch 34/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0928 - val_loss: 0.0924\n",
            "Epoch 35/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0927 - val_loss: 0.0922\n",
            "Epoch 36/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0927 - val_loss: 0.0924\n",
            "Epoch 37/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0927 - val_loss: 0.0922\n",
            "Epoch 38/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0927 - val_loss: 0.0923\n",
            "Epoch 39/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0927 - val_loss: 0.0923\n",
            "Epoch 40/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0927 - val_loss: 0.0922\n",
            "Epoch 41/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0927 - val_loss: 0.0922\n",
            "Epoch 42/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0927 - val_loss: 0.0922\n",
            "Epoch 43/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0926 - val_loss: 0.0923\n",
            "Epoch 44/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0926 - val_loss: 0.0921\n",
            "Epoch 45/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0926 - val_loss: 0.0922\n",
            "Epoch 46/50\n",
            "465/465 [==============================] - 2s 5ms/step - loss: 0.0926 - val_loss: 0.0922\n",
            "Epoch 47/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0926 - val_loss: 0.0922\n",
            "Epoch 48/50\n",
            "465/465 [==============================] - 2s 5ms/step - loss: 0.0926 - val_loss: 0.0922\n",
            "Epoch 49/50\n",
            "465/465 [==============================] - 2s 5ms/step - loss: 0.0926 - val_loss: 0.0922\n",
            "Epoch 50/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0926 - val_loss: 0.0922\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1b9768da90>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By training an autoencoder, we are really training both the encoder and the decoder at the same time.\n",
        "\n",
        "We can build an encoder and use it to compress MNIST digit images."
      ],
      "metadata": {
        "id": "h_XC_Hd-AvfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Model(inputs, encoded)\n",
        "X_test_encoded = encoder.predict(X_test)"
      ],
      "metadata": {
        "id": "QdQxABokAyMn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can confirm the 784 pixel data points are now compressed into 32 data points."
      ],
      "metadata": {
        "id": "VpBovvatAlHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_encoded[0].shape\n",
        "# You can see that the data is compressed by using an encoder."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqfIIQtpAZI8",
        "outputId": "016c014e-27b2-4e4d-ec8f-446fff2ea659"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32,)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s also build a decoder so that we can decompress the compressed image to the original image size. The decoder takes 32 data points as its input (the size of encoded data)."
      ],
      "metadata": {
        "id": "aCAAp3NtBKm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_inputs = layers.Input(shape=(32,))\n",
        "decoder = Model(decoder_inputs, dec_fc(decoder_inputs))\n",
        "# decode the encoded test data\n",
        "X_test_decoded = decoder.predict(X_test_encoded)"
      ],
      "metadata": {
        "id": "bjJwqcEsBOuI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_decoded[0].shape\n",
        "# The shape of each image is now 28*28, which is the same as original images from the dataset."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9EI--diBxoK",
        "outputId": "09ffd79b-7e03-4272-d3f1-dd6ed4ca79ae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784,)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.array(X_test_decoded[0], dtype='float').reshape((28, 28)), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "mS7hdLEGLKPk",
        "outputId": "339fa5ab-eefe-4c91-8021-8a17f38cb9bc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the image is well reconstructed by the decoder."
      ],
      "metadata": {
        "id": "ky3-ahPgPwh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolutional Autoencoder\n",
        "We could add more layers to make the network deeper to improve the performance. But since we are working on images, we could make use of convolutional neural network to improve the quality of compression and decompression."
      ],
      "metadata": {
        "id": "14QhJ0JIP28_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_convolutional_autoencoder():\n",
        "    # encoding\n",
        "    inputs = layers.Input(shape=(28, 28, 1))\n",
        "    x = layers.Conv2D(16, 3, activation='relu', padding='same')(inputs)\n",
        "    x = layers.MaxPooling2D(padding='same')(x)\n",
        "    x = layers.Conv2D( 8, 3, activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D(padding='same')(x)\n",
        "    x = layers.Conv2D( 8, 3, activation='relu', padding='same')(x)\n",
        "    encoded = layers.MaxPooling2D(padding='same')(x)    \n",
        "    \n",
        "    # decoding\n",
        "    x = layers.Conv2D( 8, 3, activation='relu', padding='same')(encoded)\n",
        "    x = layers.UpSampling2D()(x)\n",
        "    x = layers.Conv2D( 8, 3, activation='relu', padding='same')(x)\n",
        "    x = layers.UpSampling2D()(x)\n",
        "    x = layers.Conv2D(16, 3, activation='relu')(x) # <= padding='valid'!\n",
        "    x = layers.UpSampling2D()(x)\n",
        "    decoded = layers.Conv2D(1, 3, activation='sigmoid', padding='same')(x)\n",
        "    \n",
        "    # autoencoder\n",
        "    autoencoder = Model(inputs, decoded)\n",
        "    autoencoder.compile(optimizer='adam', \n",
        "                        loss='binary_crossentropy')\n",
        "    return autoencoder\n",
        "# create a convolutional autoencoder\n",
        "autoencoder = make_convolutional_autoencoder()"
      ],
      "metadata": {
        "id": "VeUwVwBSPvyK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we reshape the image data to the format the convolutional autoencoder expects for training."
      ],
      "metadata": {
        "id": "sy4Z7zHkS5-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape the flattened images to 28x28 with 1 channel\n",
        "X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "X_valid = X_valid.reshape(-1, 28, 28, 1)\n",
        "X_test  = X_test.reshape(-1, 28, 28, 1)\n",
        "autoencoder.fit(X_train, X_train, \n",
        "                epochs=50, \n",
        "                batch_size=128, \n",
        "                validation_data=(X_valid, X_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvOgR5D7KL8t",
        "outputId": "7a6c9697-dfb1-4b20-a08a-3de2488b07e7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "465/465 [==============================] - 15s 14ms/step - loss: 0.2205 - val_loss: 0.1562\n",
            "Epoch 2/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1425 - val_loss: 0.1327\n",
            "Epoch 3/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1272 - val_loss: 0.1229\n",
            "Epoch 4/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1200 - val_loss: 0.1180\n",
            "Epoch 5/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1157 - val_loss: 0.1139\n",
            "Epoch 6/50\n",
            "465/465 [==============================] - 6s 13ms/step - loss: 0.1126 - val_loss: 0.1115\n",
            "Epoch 7/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1103 - val_loss: 0.1096\n",
            "Epoch 8/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1085 - val_loss: 0.1081\n",
            "Epoch 9/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1071 - val_loss: 0.1066\n",
            "Epoch 10/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1059 - val_loss: 0.1057\n",
            "Epoch 11/50\n",
            "465/465 [==============================] - 6s 12ms/step - loss: 0.1049 - val_loss: 0.1045\n",
            "Epoch 12/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1040 - val_loss: 0.1037\n",
            "Epoch 13/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1033 - val_loss: 0.1028\n",
            "Epoch 14/50\n",
            "465/465 [==============================] - 6s 12ms/step - loss: 0.1025 - val_loss: 0.1023\n",
            "Epoch 15/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1018 - val_loss: 0.1013\n",
            "Epoch 16/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1010 - val_loss: 0.1005\n",
            "Epoch 17/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1005 - val_loss: 0.0998\n",
            "Epoch 18/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1001 - val_loss: 0.0997\n",
            "Epoch 19/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0995 - val_loss: 0.0993\n",
            "Epoch 20/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0992 - val_loss: 0.0985\n",
            "Epoch 21/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0988 - val_loss: 0.0988\n",
            "Epoch 22/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0984 - val_loss: 0.0977\n",
            "Epoch 23/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 24/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0978 - val_loss: 0.0971\n",
            "Epoch 25/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0975 - val_loss: 0.0970\n",
            "Epoch 26/50\n",
            "465/465 [==============================] - 6s 12ms/step - loss: 0.0972 - val_loss: 0.0971\n",
            "Epoch 27/50\n",
            "465/465 [==============================] - 6s 12ms/step - loss: 0.0969 - val_loss: 0.0963\n",
            "Epoch 28/50\n",
            "465/465 [==============================] - 6s 12ms/step - loss: 0.0967 - val_loss: 0.0962\n",
            "Epoch 29/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0964 - val_loss: 0.0963\n",
            "Epoch 30/50\n",
            "465/465 [==============================] - 6s 12ms/step - loss: 0.0962 - val_loss: 0.0955\n",
            "Epoch 31/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0959 - val_loss: 0.0956\n",
            "Epoch 32/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0956 - val_loss: 0.0956\n",
            "Epoch 33/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0955 - val_loss: 0.0949\n",
            "Epoch 34/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0953 - val_loss: 0.0950\n",
            "Epoch 35/50\n",
            "465/465 [==============================] - 6s 12ms/step - loss: 0.0950 - val_loss: 0.0947\n",
            "Epoch 36/50\n",
            "465/465 [==============================] - 6s 12ms/step - loss: 0.0948 - val_loss: 0.0943\n",
            "Epoch 37/50\n",
            "465/465 [==============================] - 6s 12ms/step - loss: 0.0947 - val_loss: 0.0942\n",
            "Epoch 38/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0944 - val_loss: 0.0939\n",
            "Epoch 39/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0943 - val_loss: 0.0941\n",
            "Epoch 40/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0941 - val_loss: 0.0935\n",
            "Epoch 41/50\n",
            "465/465 [==============================] - 6s 12ms/step - loss: 0.0940 - val_loss: 0.0934\n",
            "Epoch 42/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0938 - val_loss: 0.0933\n",
            "Epoch 43/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0936 - val_loss: 0.0933\n",
            "Epoch 44/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0934 - val_loss: 0.0930\n",
            "Epoch 45/50\n",
            "465/465 [==============================] - 6s 12ms/step - loss: 0.0933 - val_loss: 0.0929\n",
            "Epoch 46/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0931 - val_loss: 0.0929\n",
            "Epoch 47/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0929 - val_loss: 0.0930\n",
            "Epoch 48/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0928 - val_loss: 0.0923\n",
            "Epoch 49/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0926 - val_loss: 0.0924\n",
            "Epoch 50/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0925 - val_loss: 0.0920\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1b975b9390>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We just want to see the quality of compression/decompression, for which we do not need to build separate encoder and decoder models. So, we simply feed forward test images to see how the restored digits look like."
      ],
      "metadata": {
        "id": "7z_-9nRcTXD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_decoded = autoencoder.predict(X_test)"
      ],
      "metadata": {
        "id": "uCSv-i_GTWO3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.array(X_test_decoded[0], dtype='float').reshape((28, 28)), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "uKifMldKTlS7",
        "outputId": "dd7232dd-a47e-4eb2-90bd-a10f91ef4695"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although not perfect, the restored digit look better than the ones restored by the simple autoencoder."
      ],
      "metadata": {
        "id": "vUejs94TTuTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##What about the Noise Reduction?\n",
        "Letâ€™s try noise reduction effect using the convolutional autoencoder. We add random noises to the MINST image data and use them as input for training."
      ],
      "metadata": {
        "id": "fM_rtAGmUFFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise(x, noise_factor=0.2):\n",
        "    x = x + np.random.randn(*x.shape) * noise_factor\n",
        "    x = x.clip(0., 1.)\n",
        "    return x\n",
        "    \n",
        "X_train_noisy = add_noise(X_train)\n",
        "X_valid_noisy = add_noise(X_valid)\n",
        "X_test_noisy  = add_noise(X_test)"
      ],
      "metadata": {
        "id": "BdJvEnHSTu1N"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train a new autoencoder with the noisy data as input and the original data as expected output."
      ],
      "metadata": {
        "id": "63_wa5JxUKVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder = make_convolutional_autoencoder()\n",
        "autoencoder.fit(X_train_noisy, X_train, \n",
        "                epochs=50, \n",
        "                batch_size=128, \n",
        "                validation_data=(X_valid_noisy, X_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zetTNBvUH9q",
        "outputId": "f6a07c2c-1e4f-43a6-c905-645abb7bafec"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "465/465 [==============================] - 7s 14ms/step - loss: 0.2211 - val_loss: 0.1543\n",
            "Epoch 2/50\n",
            "465/465 [==============================] - 6s 14ms/step - loss: 0.1411 - val_loss: 0.1322\n",
            "Epoch 3/50\n",
            "465/465 [==============================] - 6s 12ms/step - loss: 0.1281 - val_loss: 0.1241\n",
            "Epoch 4/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1215 - val_loss: 0.1189\n",
            "Epoch 5/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1176 - val_loss: 0.1158\n",
            "Epoch 6/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1146 - val_loss: 0.1135\n",
            "Epoch 7/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1126 - val_loss: 0.1112\n",
            "Epoch 8/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1109 - val_loss: 0.1107\n",
            "Epoch 9/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1096 - val_loss: 0.1085\n",
            "Epoch 10/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1084 - val_loss: 0.1078\n",
            "Epoch 11/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1074 - val_loss: 0.1065\n",
            "Epoch 12/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1066 - val_loss: 0.1057\n",
            "Epoch 13/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1058 - val_loss: 0.1051\n",
            "Epoch 14/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1051 - val_loss: 0.1043\n",
            "Epoch 15/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1045 - val_loss: 0.1040\n",
            "Epoch 16/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1039 - val_loss: 0.1030\n",
            "Epoch 17/50\n",
            "465/465 [==============================] - 7s 14ms/step - loss: 0.1035 - val_loss: 0.1026\n",
            "Epoch 18/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1029 - val_loss: 0.1026\n",
            "Epoch 19/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1026 - val_loss: 0.1020\n",
            "Epoch 20/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1022 - val_loss: 0.1016\n",
            "Epoch 21/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.1019 - val_loss: 0.1014\n",
            "Epoch 22/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1015 - val_loss: 0.1008\n",
            "Epoch 23/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1012 - val_loss: 0.1012\n",
            "Epoch 24/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1008 - val_loss: 0.1004\n",
            "Epoch 25/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1006 - val_loss: 0.0997\n",
            "Epoch 26/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1002 - val_loss: 0.0995\n",
            "Epoch 27/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.1000 - val_loss: 0.0993\n",
            "Epoch 28/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0997 - val_loss: 0.0989\n",
            "Epoch 29/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0995 - val_loss: 0.0988\n",
            "Epoch 30/50\n",
            "465/465 [==============================] - 5s 12ms/step - loss: 0.0993 - val_loss: 0.0985\n",
            "Epoch 31/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0991 - val_loss: 0.0980\n",
            "Epoch 32/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0989 - val_loss: 0.0986\n",
            "Epoch 33/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0986 - val_loss: 0.0980\n",
            "Epoch 34/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0985 - val_loss: 0.0977\n",
            "Epoch 35/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0984 - val_loss: 0.0982\n",
            "Epoch 36/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0981 - val_loss: 0.0981\n",
            "Epoch 37/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0980 - val_loss: 0.0978\n",
            "Epoch 38/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0979 - val_loss: 0.0972\n",
            "Epoch 39/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0978 - val_loss: 0.0972\n",
            "Epoch 40/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0976 - val_loss: 0.0969\n",
            "Epoch 41/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0976 - val_loss: 0.0969\n",
            "Epoch 42/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0973 - val_loss: 0.0968\n",
            "Epoch 43/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0972 - val_loss: 0.0970\n",
            "Epoch 44/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0972 - val_loss: 0.0965\n",
            "Epoch 45/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0971 - val_loss: 0.0970\n",
            "Epoch 46/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0970 - val_loss: 0.0964\n",
            "Epoch 47/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0968 - val_loss: 0.0962\n",
            "Epoch 48/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0968 - val_loss: 0.0961\n",
            "Epoch 49/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0967 - val_loss: 0.0960\n",
            "Epoch 50/50\n",
            "465/465 [==============================] - 5s 11ms/step - loss: 0.0966 - val_loss: 0.0962\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1b972add90>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the training, the autoencoder learns to extract important features from input images and ignores the image noises because the labels have no noises.\n",
        "\n",
        "Letâ€™s pass the noisy test images to the autoencoder to see the restored images."
      ],
      "metadata": {
        "id": "eglb_h67UPy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_decoded = autoencoder.predict(X_test_noisy)"
      ],
      "metadata": {
        "id": "lqmaHtpgUPas"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.array(X_test_noisy[0], dtype='float').reshape((28, 28)), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "EN6DD1qWUWpA",
        "outputId": "2e540d40-48d2-4bff-9a7c-10ba9c73e4e3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.array(X_test_decoded[0], dtype='float').reshape((28, 28)), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "F-jgkfyLUZj9",
        "outputId": "71a5a576-6d2b-4382-8cde-bb3d93e16531"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOWklEQVR4nO3db4xV9Z3H8c+XgTEIhKDsTiaUCNuQCJEs6Eg2Flc3tY2LD7SJIeVBwyZENCkLTfpgjfsAY3xgNrbNxkTMoKR007VBqRFNo1BsovVB42BQ+SOFNWghw7+AzgAqDnz3wRzNqHN+Z7jn3Hvu8H2/ksncOd/53fP1Oh/Oveffz9xdAK58E+puAEBrEHYgCMIOBEHYgSAIOxDExFauzMzY9Q80mbvbaMtLbdnN7E4zO2Bmh8zswTLPBaC5rNHj7GbWIemvkn4g6YiktyStcPd9iTFs2YEma8aWfYmkQ+7+gbtfkPQ7SXeXeD4ATVQm7LMk/W3Ez0eyZV9jZqvNrM/M+kqsC0BJTd9B5+69knol3sYDdSqzZT8qafaIn7+TLQPQhsqE/S1J88xsrpl1SvqxpG3VtAWgag2/jXf3ITNbI+lVSR2SNrn73so6A1Cphg+9NbQyPrMDTdeUk2oAjB+EHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBNHwlM3RTJo0KbfW0dGRHDs0NFSqDlShVNjN7LCkQUkXJQ25e08VTQGoXhVb9n9x91MVPA+AJuIzOxBE2bC7pO1mtsvMVo/2C2a22sz6zKyv5LoAlGDu3vhgs1nuftTM/l7SDkn/7u6vJ36/8ZXVjB10GC/c3UZbXmrL7u5Hs+8nJL0gaUmZ5wPQPA2H3cymmNm0Lx9L+qGkPVU1BqBaZfbGd0l6wcy+fJ7/dfdXKumqBqm36ZJ0880359Zuu+225NiPP/44WX/uueeS9cHBwWT9woULubUyH9NwZWk47O7+gaR/rLAXAE3EoTcgCMIOBEHYgSAIOxAEYQeCKHUG3WWvrI3PoJsyZUqy/uSTT+bWli1blhw7cWL6oMeBAweS9Y8++ihZf/PNN3Nru3btSo49ePBgsn7mzJlk/eLFi8l66u+r6G9vwoTmbYuK1l22XqemnEEHYPwg7EAQhB0IgrADQRB2IAjCDgRB2IEguJV0puhuMZ999llurejy2KlTpybrixcvTtZvvPHGZP2uu+7KrZ0/fz45tui/+/jx48l66vJaSXr//fdza++8805y7HXXXZesF71uAwMDubWnn346OfaVV9JXa3/66afJejtiyw4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQXCcPfPFF18k6xs2bMitFR1rvuWWW5L1omvCz549m6x/8sknubVrr702ObarqytZLzqHoKj31LX83d3dybFLly5N1ufPn9/wuqdNm5Yc++qrrybr4xFbdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IguPsmUuXLiXre/fuza098sgjybHTp09P1oumdC66Jj3Ve9E966+++upkvbOzM1kvut9+6v7qRce6i163efPmNbzu1157LTk2df+C8apwy25mm8zshJntGbHsGjPbYWYHs+8zmtsmgLLG8jb+15Lu/MayByXtdPd5knZmPwNoY4Vhd/fXJZ3+xuK7JW3OHm+WdE/FfQGoWKOf2bvcvT97fExS7gnWZrZa0uoG1wOgIqV30Lm7pyZsdPdeSb1Se0/sCFzpGj30dtzMuiUp+36iupYANEOjYd8maWX2eKWkF6tpB0CzFL6NN7NnJd0uaaaZHZG0XtJjkraY2SpJH0pa3swm20HqevdTp04lx548ebLqdipz7ty5ZN1s1Km+v1I0h3pHR0du7aqrrkqO7e/vT9aL5kjv6+vLrT3++OPJsUXnXYxHhWF39xU5pe9X3AuAJuJ0WSAIwg4EQdiBIAg7EARhB4LgEtcKFB0Camdley86RJU6dFc0lfUdd9yRrH/++efJ+vr163NrRYccr0Rs2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCI6zI6nscfjUJa73339/cuzChQuT9UOHDiXru3btyq2N53MjGsWWHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeC4Dg7mmrGjPwJfu+9995Sz93b25usDw4Olnr+Kw1bdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IguPsKCV1vbokrV27Nrc2Z86c5Nhjx44l61u2bEnWI16znlK4ZTezTWZ2wsz2jFj2sJkdNbPd2dey5rYJoKyxvI3/taQ7R1n+K3dflH39odq2AFStMOzu/rqk0y3oBUATldlBt8bM3s3e5ueeAG1mq82sz8z6SqwLQEmNhn2DpO9KWiSpX9Iv8n7R3XvdvcfdexpcF4AKNBR2dz/u7hfd/ZKkjZKWVNsWgKo1FHYz6x7x448k7cn7XQDtofA4u5k9K+l2STPN7Iik9ZJuN7NFklzSYUnpG4Bj3ErNry5JCxYsSNZXrVqVWxsaGkqOffTRR5P106fZb3w5CsPu7itGWfxME3oB0EScLgsEQdiBIAg7EARhB4Ig7EAQXOIa3IQJ6X/vZ86cmaxv3LgxWZ8+fXpu7Y033kiOff7555N1LmG9PGzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIjrMHN3ny5GR9zZo1yfoNN9yQrJ87dy63tm7duuRYplyuFlt2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiC4+xXuKJbQS9cuDBZf+CBB5L1zs7OZH3Tpk25tf379yfHolps2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCGvlvbfNjBt9t1jqvu2S9NJLLyXrt956a7I+MDCQrM+dOze3xpTLzeHuo55cUbhlN7PZZvYnM9tnZnvNbF22/Boz22FmB7PvM6puGkB1xvI2fkjSz919gaR/kvRTM1sg6UFJO919nqSd2c8A2lRh2N29393fzh4PStovaZakuyVtzn5ts6R7mtUkgPIu69x4M5sjabGkv0jqcvf+rHRMUlfOmNWSVjfeIoAqjHlvvJlNlbRV0s/c/Wt7ZXx4L9+oO9/cvdfde9y9p1SnAEoZU9jNbJKGg/5bd/99tvi4mXVn9W5JJ5rTIoAqFL6Nt+FrJJ+RtN/dfzmitE3SSkmPZd9fbEqHKNTR0ZFbW758eXLsTTfdVGrdqUtYJenMmTOlnh/VGctn9u9J+omk98xsd7bsIQ2HfIuZrZL0oaT0XxWAWhWG3d3/LCnvDgjfr7YdAM3C6bJAEIQdCIKwA0EQdiAIwg4EwSWuV4Du7u7c2vbt25NjFyxYkKyfP38+WZ8/f36yfuTIkWQd1Wv4ElcAVwbCDgRB2IEgCDsQBGEHgiDsQBCEHQiCKZvHgaJpl9euXZtbu/7665NjL168mKy//PLLyfqxY8eSdbQPtuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATH2ceBmTNnJuv33Xdfbm3ixPT/4gMHDiTrTzzxRLJedJwe7YMtOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EMZb52WdL+o2kLkkuqdfd/9vMHpZ0n6ST2a8+5O5/aFajkfX09CTrnZ2dubWBgYHk2KeeeipZ37dvX7KO8WMsJ9UMSfq5u79tZtMk7TKzHVntV+7+ePPaA1CVsczP3i+pP3s8aGb7Jc1qdmMAqnVZn9nNbI6kxZL+ki1aY2bvmtkmM5uRM2a1mfWZWV+pTgGUMuawm9lUSVsl/czdByRtkPRdSYs0vOX/xWjj3L3X3XvcPf3BE0BTjSnsZjZJw0H/rbv/XpLc/bi7X3T3S5I2SlrSvDYBlFUYdhu+tekzkva7+y9HLB85deiPJO2pvj0AVRnL3vjvSfqJpPfMbHe27CFJK8xskYYPxx2WdH9TOoQGBweT9dTtnIsuYd26dWupdbdyym+UM5a98X+WNNqNyzmmDowjnEEHBEHYgSAIOxAEYQeCIOxAEIQdCMJaeZzUzDgo24DJkycn693d3bm1U6dOJceeO3cuWedW0eOPu486xzdbdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IotXH2U9K+nDEopmS0geC69OuvbVrXxK9NarK3q5z978brdDSsH9r5WZ97XpvunbtrV37kuitUa3qjbfxQBCEHQii7rD31rz+lHbtrV37kuitUS3prdbP7ABap+4tO4AWIexAELWE3czuNLMDZnbIzB6so4c8ZnbYzN4zs911z0+XzaF3wsz2jFh2jZntMLOD2fdR59irqbeHzexo9trtNrNlNfU228z+ZGb7zGyvma3Lltf62iX6asnr1vLP7GbWIemvkn4g6YiktyStcPe2mAjczA5L6nH32k/AMLN/lnRW0m/c/YZs2X9JOu3uj2X/UM5w9/9ok94elnS27mm8s9mKukdOMy7pHkn/phpfu0Rfy9WC162OLfsSSYfc/QN3vyDpd5LurqGPtufur0s6/Y3Fd0vanD3erOE/lpbL6a0tuHu/u7+dPR6U9OU047W+dom+WqKOsM+S9LcRPx9Re8337pK2m9kuM1tddzOj6HL3/uzxMUlddTYzisJpvFvpG9OMt81r18j052Wxg+7blrr7jZL+VdJPs7erbcmHP4O107HTMU3j3SqjTDP+lTpfu0anPy+rjrAflTR7xM/fyZa1BXc/mn0/IekFtd9U1Me/nEE3+36i5n6+0k7TeI82zbja4LWrc/rzOsL+lqR5ZjbXzDol/VjSthr6+BYzm5LtOJGZTZH0Q7XfVNTbJK3MHq+U9GKNvXxNu0zjnTfNuGp+7Wqf/tzdW/4laZmG98j/n6T/rKOHnL7+QdI72dfeunuT9KyG39Z9oeF9G6skXStpp6SDkv4o6Zo26u1/JL0n6V0NB6u7pt6Wavgt+ruSdmdfy+p+7RJ9teR143RZIAh20AFBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEP8PubqeBHDw8w4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the noisy image is well recostructed."
      ],
      "metadata": {
        "id": "9Nin0EBuYwua"
      }
    }
  ]
}