{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDerBS_foQl2"
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5z34ayPNA13"
   },
   "source": [
    "# Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Training](#train)\n",
    "3. [Architectures](#architectures)\n",
    "  * [One to Many](#otm)\n",
    "  * [Many to One](#mto)\n",
    "  * [Many to Many](#mtm)\n",
    "6. [Example](#code_example)\n",
    "6. [Conclusion](#conclusion)\n",
    "7. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OotVuBgfqME"
   },
   "source": [
    "## Introduction <a name=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeHq0bAh4c2u"
   },
   "source": [
    "Traditional feed-forward neural networks take in a fixed amount of input data all at the same time and produce a fixed amount of output each time. However, in some context in machine learning we want to have more flexibility in the types of data that our model can process. therefore, we move to this idea of recurrent neural networks (RNN). A recurrent neural network is a special type of an artificial neural network adapted to work for time series data or data that involves sequences; Meaning, RNNs do not consume all the input data at once. Instead, they take them in one at a time and in a sequence. At each step, the RNN does a series of calculations before producing an output. The output, known as the hidden state, is then combined with the next input in the sequence to produce another output. This process continues until the model is programmed to finish or the input sequence ends. To sum up, RNNs have the concept of memory that helps them store the states or information of previous inputs to generate the next output of the sequence.\n",
    "\n",
    "</br>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://dezyre.gumlet.io/images/blog/rnn-vs-cnn-the-difference/image_4953448241632753191724.png?w=750&dpr=2.0\"\n",
    "   width=\"500\" \n",
    "     height=\"500\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYV4Z8O_fqMF"
   },
   "source": [
    "## Training <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-thmGRK44qD6"
   },
   "source": [
    "We can think about RNNs in two ways. one is this concept of having a hidden state that feeds back at itself recurrently. The other one is to think about unrolling this computational graph for multiple time steps. This would help understanding the recurrent network easier.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg\"\n",
    "   width=\"500\" \n",
    "     height=\"300\"/>\n",
    "</p>\n",
    "\n",
    " $x_t$ is the input at time step t. To keep things simple we assume that $x_t$ is a scalar value with a single feature. You can extend this idea to a d-dimensional feature vector.\n",
    " </br>\n",
    " $o_t$ is the output of the network at time step t. We can produce multiple outputs in the network but for this example we assume that there is one output.\n",
    " </br>\n",
    " $h_t$ vector stores the values of the hidden states at time t. This is also called the current context. $h_0$ vector is initialized to zero.\n",
    " </br>\n",
    " $w_t$ is weight matrix.\n",
    "</br>\n",
    " At every time step we can unfold the network for k time steps to get the output at time step k+1. The unfolded network is very similar to the feedforward neural network.\n",
    " </br>\n",
    " Now that we are seeing recurrent neural network as an feedforward neural network with k step, we can easily compute the outputs.\n",
    "\n",
    " <center>\n",
    " $h_t = f_w(h_{t-1}, x_t) = tanh(w_{hh}h_{t-1} + w_{xh}x_t)$\n",
    " </br>\n",
    " $y_t = w_{yh}h_t$\n",
    " </center>\n",
    " \n",
    " During training, for each piece of training data we will have a corresponding ground-truth label that we want the model to output. After receiving these outputs, we will calculate the loss of that process, which measures how far off, the modelâ€™s output is from the correct answer. Using this loss, we can calculate the gradient of the loss function for back-propagation.\n",
    "With the gradient that we just obtained, we can update the weights in the model accordingly. Combined with the forward pass, back-propagation is looped over and again, allowing the model to become more accurate with its outputs each time as the weight matrices values are modified to pick out the patterns of the data.\n",
    "\n",
    "Although it may look as if each RNN cell is using a different weight as shown in the graphics, all of the weights are actually the same as that RNN cell is essentially being re-used throughout the process. This may lead to one of RNNs disadvantages which is the vanishing gradient problem, where the gradients used to compute the weight update may get very close to zero due to multiplication of the same matrix over and over again which prevents the network from learning new weights. The deeper the network, the more pronounced is this problem.\n",
    "\n",
    "The pseudo-code for training is given below. The value of k which is the recursion factor can be selected by the user for training. In the pseudo-code below $p_t$ is the target value at time step t:\n",
    "\n",
    "Repeat till stopping criterion is met:\n",
    "</br>\n",
    "Set all h to zero.\n",
    "</br>\n",
    "Repeat for t = 0 to k\n",
    "</br>\n",
    "Forward propagate the network over the unfolded network for k time steps to compute all h and y.\n",
    "</br>\n",
    "Compute the error as: $error = y_{k} - p_{k}$\n",
    "</br>\n",
    "Backpropagate the error across the unfolded network and update the weights.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEn8GUyrfqMO"
   },
   "source": [
    "## Architectures <a name=\"architectures\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjLahM_Vbqz2"
   },
   "source": [
    "RNNs are really flexible and can adapt to your needs. As you will see in the images below, your input and output size can come in different forms, yet they can still be fed and extracted from the RNN model. There are different types of recurrent neural networks with varying architectures that are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4ZBEZCDfqMO"
   },
   "source": [
    "### One to Many <a name=\"otm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcGXgwZ_fqMP"
   },
   "source": [
    "This type of neural network has a input which is an object of fixed size like an image and the output is a sequence of variable lenght, such as a caption where diffrent captions might have diffrent number of words, so our output needs to be variable at lenght. \n",
    "<br>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.simplilearn.com/ice9/free_resources_article_thumb/One_to_Many_RNN.png\"\n",
    "   width=\"400\" \n",
    "     height=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoqWqW24fqMQ"
   },
   "source": [
    "### Many to One <a name=\"mto\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4L4mL1nt5d99"
   },
   "source": [
    "This RNN takes a sequence of inputs that could be variably sized like a text and generates a single output. Sentiment analysis is a good example of this kind of network where a given sentence can be classified as expressing positive or negative sentiments or in a computer vision contex, you might imagine taking as input, a video which might have variable number of frames and we want to read this entire video of potentioally variable lenght and at the end, make a classification decision about the kind of activity that is going on in that video.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.simplilearn.com/ice9/free_resources_article_thumb/Many_to_One_RNN.png\"\n",
    "   width=\"400\" \n",
    "     height=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UstdBiu75eKS"
   },
   "source": [
    "### Many to Many <a name=\"mtm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxixV6SG6ldE"
   },
   "source": [
    "This RNN takes a sequence of inputs and generates a sequence of outputs. Machine translation is one of the examples where our input might be some sentence in English, which could have a variable lenght and our output is the same sentence but in French, which also could have a variable length and crucially the length of the English sentence might be diffrent from the lenght of the French sentence so we need some models that have the capacity to accept both variable length sequences on the input and the output.\n",
    "\n",
    "We might also consider problems in computer vision contex, where our input is variably length like a video sequence with variable number of frames and we want to make a decision for each element of that input sequence. which in the context of video, is making a classification decision along every frame of that video.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.simplilearn.com/ice9/free_resources_article_thumb/Many_to_Many_RNN.png\"\n",
    "   width=\"400\" \n",
    "     height=\"400\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw above, RNNs are like a general paradigm for handling variable sized sequenced data that allow us to capture all of these diffrent types of setups in our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29Hx95TFfqMU"
   },
   "source": [
    "## Example <a name=\"code_example\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdJiNHvH6nWn"
   },
   "source": [
    "In this example we will be implementing a simple RNN character model with PyTorch to familiarize ourselves with the PyTorch library and get started with RNNs. \n",
    "In this implementation, we will be building a model that can complete your sentence based on a few characters or a word used as input.\n",
    "\n",
    "We will start off by installing and importing the main packages that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch\n",
    "# !pip3 install numpy\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to set our device first. we would use gpu if available and cpu if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define the sentences that we want our model to output when fed with the first word or the first few characters and create a dictionary out of all the characters that we have in the sentences and map them to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['hey how are you','good i am fine','have a nice day']\n",
    "chars = set(''.join(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {char: ind for ind, char in int2char.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will be padding our input sentences to ensure that all the sentences are of the sample length. While RNNs are typically able to take in variably sized inputs, we will usually want to feed training data in batches to speed up the training process. In order to used batches to train on our data, we'll need to ensure that each sequence within the input data are of equal size.\n",
    "\n",
    "Therefore, in most cases, padding can be done by filling up sequences that are too short with 0 values and trimming sequences that are too long. In our case, we'll be finding the length of the longest sequence and padding the rest of the sentences with blank spaces to match that length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = len(max(text, key=len))\n",
    "\n",
    "for i in range(len(text)):\n",
    "    while len(text[i])<maxlen:\n",
    "        text[i] += ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are going to predict the next character in the sequence at each time step, we will have to divide each sentence into Input data and Target. Also, we should be careful that the last input character should be excluded as it does not need to be fed into the model.\n",
    "\n",
    "Our target is one time-step ahead of the Input data as this will be the answer for the model at each time step corresponding to the input data.\n",
    "\n",
    "At the end we convert our input and target sequences to sequences of integers instead of characters by mapping them using the dictionaries we created before. This will allow us to one-hot-encode our input sequence subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: hey how are yo\n",
      "Target Sequence: ey how are you\n",
      "Input Sequence: good i am fine\n",
      "Target Sequence: ood i am fine \n",
      "Input Sequence: have a nice da\n",
      "Target Sequence: ave a nice day\n"
     ]
    }
   ],
   "source": [
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    input_seq.append(text[i][:-1])\n",
    "    target_seq.append(text[i][1:])\n",
    "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))\n",
    "\n",
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we encode our input sequence into one-hot vectors and convert them to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (3, 14, 17) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n"
     ]
    }
   ],
   "source": [
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features\n",
    "\n",
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
    "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))\n",
    "\n",
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start building our own neural network model, we can define a class that inherits PyTorchâ€™s base class (nn.module) for all neural network modules. After doing so, we can start defining some variables and also the layers for our model under the constructor. For this model, we wll only be using one layer of RNN followed by a fully connected layer. The fully connected layer will be in-charge of converting the RNN output to our desired output shape.\n",
    "\n",
    "We also have to define the forward pass function under forward() as a class method. The order the forward function is sequentially executed, therefore have to pass the inputs and the zero-initialized hidden state through the RNN layer first, before passing the RNN outputs to the fully-connected layer.\n",
    "\n",
    "The last method that we have to define is the method that we called earlier to initialize the hidden state - init_hidden(). This basically creates a tensor of zeros in the shape of our hidden states.\n",
    "\n",
    "Then we create an instance of our model and initialize the hyperparameters and start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
    "model = model.to(device)\n",
    "\n",
    "n_epochs = 100\n",
    "lr=0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100............. Loss: 2.5228\n",
      "Epoch: 20/100............. Loss: 2.1118\n",
      "Epoch: 30/100............. Loss: 1.7116\n",
      "Epoch: 40/100............. Loss: 1.3229\n",
      "Epoch: 50/100............. Loss: 0.9832\n",
      "Epoch: 60/100............. Loss: 0.7112\n",
      "Epoch: 70/100............. Loss: 0.5081\n",
      "Epoch: 80/100............. Loss: 0.3617\n",
      "Epoch: 90/100............. Loss: 0.2649\n",
      "Epoch: 100/100............. Loss: 0.2016\n"
     ]
    }
   ],
   "source": [
    "input_seq = input_seq.to(device)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    #input_seq = input_seq.to(device)\n",
    "    output, hidden = model(input_seq)\n",
    "    output = output.to(device)\n",
    "    target_seq = target_seq.to(device)\n",
    "    loss = criterion(output, target_seq.view(-1).long())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, character):\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character)\n",
    "    character = character.to(device)\n",
    "    \n",
    "    out, hidden = model(character)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return int2char[char_ind], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, out_len, start='hey'):\n",
    "    model.eval() \n",
    "    start = start.lower()\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good i am fine '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(model, 15, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model is able to come up with the sentence â€˜good i am fine â€˜ if we feed it with the words â€˜goodâ€™, achieving what we intended for it to do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a name=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we discuss:\n",
    "<br>\n",
    "- How a recurrent neural network handles sequential data\n",
    "    <br>\n",
    "- Unfolding a recurrent neural network\n",
    "    <br>\n",
    "- Training and back propagation in time\n",
    "    <br>\n",
    "- Various architectures and variants of RNN\n",
    "    <br>\n",
    "- Simple example of a vanilla RNN\n",
    "\n",
    "This is just the tip of the iceberg when it comes to Recurrent Neural Networks. While the vanilla RNN is rarely used in solving NLP or sequential problems, having a good grasp of the basic concepts of RNNs will definitely aid in your understanding as you move towards the more popular GRUs and LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BDIzkJhfqMV"
   },
   "source": [
    "## References <a name=\"references\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAfujAPs6n1B"
   },
   "source": [
    "[Recurrent Neural Networks Lecture, Stanford University School of Engineering](https://www.youtube.com/c/stanfordengineering)\n",
    "</br>\n",
    "[Recurrent Neural Network (RNN) Tutorial: Types, Examples, LSTM and More](https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn)\n",
    "</br>\n",
    "[RNN walkthrough](https://github.com/gabrielloye/RNN-walkthrough)\n",
    "</br>\n",
    "[An Introduction To Recurrent Neural Networks And The Math That Powers Them](https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/)\n",
    "</br>\n",
    "[A Tour of Recurrent Neural Network Algorithms for Deep Learning](https://machinelearningmastery.com/recurrent-neural-network-algorithms-for-deep-learning/)\n",
    "</br>\n",
    "[A Beginnerâ€™s Guide on Recurrent Neural Networks with PyTorch](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "index.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
