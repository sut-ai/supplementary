{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#Introduction)\n",
    "- [Definition](#Definition)\n",
    "- [Architecture](#Architecture)\n",
    "  - [MapReduce](#MapReduce)\n",
    "  - [Apache Hadoop](#Apache-Hadoop)\n",
    "  - [Apache Spark](#Apache-Spark)\n",
    "- [Further Reading](#Further-Reading)\n",
    "- [References](#References)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Have you ever wondered how much data (in the form of texts, phone calls, videos, emails, searches, etc.) do we generate only when using our smartphones? Approximately 40 exabytes (40 million terabytes) of data gets generated every month by a single smartphone user! Now have fun multiplying it by 5 billion (and counting) users :)\n",
    "<br /><br />\n",
    "Traditional computing systems are no longer capable of handling this amount of data which we call Big Data.\n",
    "In the following sections we learn what data should we call Big Data, learn about some applications, and get familiar with some tools and techniques which are utilized to store and process that data.\n",
    "<br /><br />\n",
    "In the end, some extra links are provided for [further studying](#Further-Reading) the subjects discussed.\n",
    "\n",
    "<center>\n",
    "<img src=\"./assets/Big-Data-Intro.jpg\" alt=\"Big Data\" width=\"60%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "A piece of data is classified as Big Data regarding to the concept of a bunch of **V**s. In order to understand them, consider health care industry and medical records as an example:\n",
    "  - **Volume**: Hospitals and clinics collect massive volume of data (more than 2000 exabytes) every year.\n",
    "  - **Velocity**: This data is generated at a very high speed.\n",
    "  - **Variety**: Various data types are collected such as structured (e.g. excel records), semi-structured (e.g. log files) and unstructured data (e.g. X-ray images), which complicates detecting and categorizing important and related data to our goal (which can be fast disease detection or better treatment).\n",
    "  - and other characteristics such as **Value**, **Veracity** and **Variability** which you can learn more about [here](https://www.motivaction.nl/en/news/blog/big-data-the-6-vs-you-need-to-look-at-for-important-insights).\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "Various frameworks are developed to ease the processing and storage of Big Data such as Cassandra, Apache Hadoop, and Apache Spark. But first, let's talk about a rather new parallel processing model designed to deal with some of Big Data challenges called **MapReduce**.\n",
    "\n",
    "### MapReduce\n",
    "MapReduce is a software framework and programming model used for processing huge amounts of data which comprises two phases, **Map** and **Reduce**. Map tasks deal with splitting and mapping of data while Reduce tasks shuffle and reduce the data. The following image pretty much sums up the MapReduce architecture in _Hadoop_.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.guru99.com/images/Big_Data/061114_0930_Introductio1.png\" alt=\"MapReduce architecture\" width=\"60%\"/>\n",
    "</center>\n",
    "\n",
    "### Apache Hadoop\n",
    "Apache Hadoop is a software platform that manages data processing and storage for big data applications. Hadoop works by distributing large data sets and analytics jobs across nodes in a computing cluster, breaking them down into smaller workloads that can be run in parallel. Hadoop is capable of running MapReduce programs written in various languages.\n",
    "<br />\n",
    "The programs of Map Reduce in cloud computing are parallel in nature, thus are very useful for performing large-scale data analysis using multiple machines in the cluster. The benefits of using Hadoop are scalability, resilience and flexibility.\n",
    "\n",
    "Hadoop divides the job into tasks. The types of tasks are Map tasks (Splits & Mapping) and Reduce tasks (Shuffling, Reducing) as mentioned above.\n",
    "The complete execution process (execution of Map and Reduce tasks) is controlled by two types of entities called\n",
    "  1. Jobtracker: Acts like a _master_ (responsible for complete execution of submitted job)\n",
    "  2. Multiple Task Trackers: Acts like _workers_, each of them performing the job\n",
    "\n",
    "For every job submitted for execution in the system, there is one Jobtracker that resides on Namenode and there are multiple tasktrackers which reside on Datanode.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.guru99.com/images/Big_Data/061114_0930_Introductio2.png\" alt=\"How Hadoop MapReduce Works\" width=\"60%\"/>\n",
    "</center>\n",
    "\n",
    "### Apache Spark\n",
    "One of the main challenges of Hadoop architecture is _performance_. Hadoop uses frequent reads and writes to disk to perform computations, which is time-consuming and inefficient compared to frameworks that aim to store and process data **in memory** as much as possible, like Apache Spark™.\n",
    "\n",
    "[Apache Spark™](https://spark.apache.org/) is a general-purpose & lightning fast cluster computing platform. In other words, it is an open source, wide range data processing engine. That reveals development API’s, which also qualifies data workers to accomplish streaming, machine learning or SQL workloads which demand repeated access to data sets. However, Spark can perform both batch processing and stream processing.\n",
    "The main website provides a very well overview of its capabilities. Spark also has a strong [documentation](https://spark.apache.org/docs/latest/) that helps you get started.\n",
    "\n",
    "<center>\n",
    "<img src=\"assets/Apache-Spark-Components.jpg\" alt=\"Apache Spark Ecosystem Components\" width=\"60%\"/>\n",
    "</center>\n",
    "\n",
    "Moreover, it is designed in such a way that it integrates with all the Big data tools. Like spark can access any Hadoop data source, also can run on Hadoop clusters. Furthermore, Apache Spark extends Hadoop MapReduce to the next level. That also includes iterative queries and stream processing.\n",
    "\n",
    "Although, there is one spark’s key feature that it has in-memory cluster computation capability. Also increases the processing speed of an application. [This blog](https://data-flair.training/blogs/spark-tutorial/) by Data Flair provides a comprehensive tutorial on Spark which is highly recommended for those interested.\n",
    "\n",
    "#### Experience Spark\n",
    "In this section we finally try to get our hands dirty and gain a perspective on how a spark program is written. There are many options like Scala, Java, R and Python and we naturally choose [PySpark](https://pypi.org/project/pyspark/) to proceed. Although some of the resources which we introduce provide examples in all possible languages.\n",
    "\n",
    "  - First, we recommend you installing Spark via Docker. It saves you a lot of time and energy.\n",
    "  - [This Repo](https://github.com/cluster-apps-on-docker/spark-standalone-cluster-on-docker) (among many others) offers Docker images and configuration files which can be used to create a master node and a worker node and also, run jupyter notebook and let you connect to the cluster through the exposed ports. Try to see the differences between these nodes by reading Dockerfiles, and try to find other commands (e.g. `spark-submit`) which can be useful in the Spark Shell.\n",
    "  - Run [this notebook](https://github.com/cluster-apps-on-docker/spark-standalone-cluster-on-docker/blob/master/build/workspace/pyspark.ipynb) and pay attention to every cell output.\n",
    "\n",
    "Two other great tutorials using some other docker image are provided [here](https://medium.com/@suci/running-pyspark-on-jupyter-notebook-with-docker-602b18ac4494) (Medium) and [here](https://towardsdatascience.com/apache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445) (Towards Data Science). You could also use [Tutorialspoint](https://www.tutorialspoint.com/pyspark/index.htm) to learn every feature that PySpark offers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "These links and videos below help you get started and learn about basic concepts more deeply and then install and run a framework (e.g. Spark) and experience its cool features afterwards.\n",
    "  - Research Topics and Activities:\n",
    "    - [Top 20 Latest Research Problems in Big Data and Data Science | Towards Data Science](https://towardsdatascience.com/top-20-latest-research-problems-in-big-data-and-data-science-c6fb51e03136)\n",
    "    - [MapReduce | Google Research](https://research.google/pubs/pub62/)\n",
    "  - Big Data vs. AI:\n",
    "    - [How Big Data and AI Work Together | Qlik](https://www.qlik.com/us/augmented-analytics/big-data-ai)\n",
    "  - Big Data Applications and Uses:\n",
    "    - [Big Data | Oracle](https://www.oracle.com/in/big-data/what-is-big-data/)\n",
    "    - [Big Data | Wiki](https://en.wikipedia.org/wiki/Big_data#Applications)\n",
    "  - MapReduce:\n",
    "    - [What Is MapReduce In Hadoop? | Hadoop MapReduce Tutorial | Simplilearn Youtube](https://www.youtube.com/watch?v=b-IvmXoO0bU)\n",
    "    - [MapReduce | Wiki](https://en.wikipedia.org/wiki/MapReduce)\n",
    "    - [What is MapReduce? | Talend](https://www.talend.com/resources/what-is-mapreduce/)\n",
    "  - Apache Hadoop:\n",
    "    - [Hadoop | Databricks](https://databricks.com/glossary/hadoop)\n",
    "  - Apache Spark:\n",
    "    - [Spark Tutorial For Beginners | Big Data Spark Tutorial | Apache Spark Tutorial | Simplilearn Youtube](https://www.youtube.com/watch?v=QaoJNXW6SQo)\n",
    "    - [مقدمه‌ای بر آپاچی اسپارک | ویرگول](https://vrgl.ir/P9uxu)\n",
    "    - [معرفی و آشنایی با آپاچی اسپارک | Big Data World](https://bigdataworld.ir/%D9%85%D8%B9%D8%B1%D9%81%DB%8C-%D9%88-%D8%A2%D8%B4%D9%86%D8%A7%DB%8C%DB%8C-%D8%A8%D8%A7-%D8%A2%D9%BE%D8%A7%DA%86%DB%8C-%D8%A7%D8%B3%D9%BE%D8%A7%D8%B1%DA%A9/)\n",
    "    - [Spark RDD – Introduction, Features & Operations of RDD | ِData Flair](https://data-flair.training/blogs/spark-rdd-tutorial/)\n",
    "    - [Apache Spark Use Cases in Real Time | Data Flair](https://data-flair.training/blogs/spark-use-cases/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "  - [What is Big Data? | Simplilearn Youtube](https://www.youtube.com/watch?v=bAyrObl7TYE)\n",
    "  - [What is MapReduce in Hadoop? | Guru99](https://www.guru99.com/introduction-to-mapreduce.html)\n",
    "  - [Spark Tutorial – Learn Spark Programming | Data Flair](https://data-flair.training/blogs/spark-tutorial/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
